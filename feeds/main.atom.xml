<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Contemplative Coding</title><link href="http://proycon.github.io/" rel="alternate"></link><link href="http://proycon.github.io/feeds/main.atom.xml" rel="self"></link><id>http://proycon.github.io/</id><updated>2013-11-12T14:01:01+01:00</updated><entry><title>Modelling n-grams, skipgrams and flexgrams in corpora with Colibri Core</title><link href="http://proycon.github.io/blog/2014/03/31/colibri-core/" rel="alternate"></link><updated>2013-11-12T14:01:01+01:00</updated><author><name>Maarten van Gompel (proycon)</name></author><id>tag:proycon.github.io,2014-03-31:blog/2014/03/31/colibri-core/</id><summary type="html">&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 2)&lt;/p&gt;
&lt;p&gt;Title underline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Modelling n-grams, skipgrams and flexgrams in corpora with Colibri Core
############################################
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In this first actual blog post I would like to introduce Natural Language
Processing software I developed in the scope of my Ph.D research at Radboud University Nijmegen: &lt;strong&gt;Colibri Core&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="section" id="introduction"&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 15)&lt;/p&gt;
&lt;p&gt;Title overline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
====
Introduction
====
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Everything is open-source (GPLv3) and can be obtained from:
&lt;a class="reference external" href="http://github.com/proycon/colibri-core"&gt;http://github.com/proycon/colibri-core&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation is available from &lt;a class="reference external" href="http://proycon.github.io/colibri-core/doc/"&gt;http://proycon.github.io/colibri-core/doc/&lt;/a&gt; ,
which documents installation, usage as well as the Python API.&lt;/p&gt;
&lt;p&gt;In this blog post I will merely illustrate the paradigm and the features of
Colibri Core, so you get a clear impression of the possibilities. For more I
refer to the aforementioned documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="patterns"&gt;
&lt;h2&gt;Patterns&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 29)&lt;/p&gt;
&lt;p&gt;Title overline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
=======
Patterns
=======
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Colibri Core is software to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of &lt;em&gt;pattern&lt;/em&gt; or &lt;em&gt;construction&lt;/em&gt; encompasses the following categories:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;n-gram&lt;/strong&gt;        &lt;em&gt;n&lt;/em&gt; consecutive words&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;skipgram&lt;/strong&gt;      An abstract pattern of predetermined length with one or multiple gaps (of
specific size).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;flexgram&lt;/strong&gt;      An abstract pattern without predetermined length, with one
or more gaps.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;N-gram extraction may seem fairly trivial at first, with a few lines in your
favourite scripting language, you can move a simple sliding window of size &lt;em&gt;n&lt;/em&gt;
over your corpus and store the results in some kind of hashmap. This trivial
approach however makes an unnecessarily high demand on memory resources, this
often becomes prohibitive if unleashed on large corpora. Colibri Core tries to minimise
these space requirements in several ways:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Binary representation&lt;/strong&gt;     Each word type is assigned a numeric class,
which is encoded in a compact binary format in which highly frequent classes take less space than less frequent classes. Colibri core always uses this representation rather than a full string representation, both on disk and in memory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Informed counting&lt;/strong&gt;      Counting is performed more intelligently by iteratively processing the corpus in several
passes and quickly discarding patterns that won't reach the desired occurrence threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Skipgram and flexgram extraction are computationally more demanding but have
been implemented with similar optimisations. Skipgrams are computed by
abstracting over n-grams, and flexgrams in turn are computed either by abstracting
over skipgrams, or directly from n-grams on the basis of co-occurrence information (mutual
pointwise information).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pattern-models"&gt;
&lt;h2&gt;Pattern Models&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 59)&lt;/p&gt;
&lt;p&gt;Title overline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
=====
Pattern Models
=====
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;At the heart of the sofware is the notion of pattern models. The core tool, to
be used from the command-line, is &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;colibri-patternmodeller&lt;/span&gt;&lt;/tt&gt; which enables you to
build pattern models, generate statistical reports, query for specific patterns
and relations, and manipulate models.&lt;/p&gt;
&lt;p&gt;A pattern model is simply a collection of extracted patterns (any of the three categories) and their counts from a
specific corpus. Pattern models come in two varieties:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Unindexed Pattern Model&lt;/strong&gt; - The simplest form, which simply stores the
patterns and their count.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indexed Pattern Model&lt;/strong&gt;   - The more informed form, which retains all
indices to the original corpus, at the cost of more memory/diskspace.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Indexed Pattern Model is much more powerful, and allows more statistics and
relations to be inferred.&lt;/p&gt;
&lt;p&gt;The generation of pattern models is optionally parametrised by a minimum occurrence
threshold, a maximum pattern length, and a lower-boundary on the different types
that may instantiate a skipgram (i.e. possible fillings of the gaps).&lt;/p&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h3&gt;Statistics&lt;/h3&gt;
&lt;p&gt;The statistics that are reported on are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Occurrence count&lt;/strong&gt; - The absolute number of times a pattern occurs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokens covered&lt;/strong&gt; - The absolute number of tokens in the corpus that a pattern covers. Computed as &lt;tt class="docutils literal"&gt;occurrencecount * n&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coverage&lt;/strong&gt; - The number of covered tokens, as a fraction of the total number of tokens in the original corpus.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frequency&lt;/strong&gt; - The frequency of the pattern within its category and size class, so for an ngram of size two, the frequency indicates the frequency amongst all bigrams.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Types&lt;/strong&gt; - The number of unique word types covered.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These can be reported afer querying for specific patterns, or for groups (e.g.
all bigrams). For unindexed models, coverage computation is a mere
approximation. An example output report of a small pattern model from a tiny
toy corpus is shown below:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
REPORT
----------------------------------
                            PATTERNS    TOKENS  COVERAGE     TYPES
Total:                             -       340         -       177
Uncovered:                         -       175    0.5147       136
Covered:                          69       165    0.4853        41

  CATEGORY N (SIZE)   PATTERNS    TOKENS  COVERAGE     TYPES OCCURRENCES
       all       all        69       165    0.4853        41         243
       all         1        40       165    0.4853        40         165
       all         2        11        26    0.0765        13          26
       all         3         7        17    0.0500         9          19
       all         4         5        10    0.0294         9          14
       all         5         5         9    0.0265         9          17
       all         6         1         2    0.0059         6           2
    n-gram       all        62       165    0.4853        40         215
    n-gram         1        40       165    0.4853        40         165
    n-gram         2        11        26    0.0765        13          26
    n-gram         3         5        12    0.0353         8          12
    n-gram         4         3         6    0.0176         6           6
    n-gram         5         2         4    0.0118         6           4
    n-gram         6         1         2    0.0059         6           2
  skipgram       all         7         7    0.0206         6          28
  skipgram         3         2         7    0.0206         4           7
  skipgram         4         2         4    0.0118         4           8
  skipgram         5         3         5    0.0147         5          13
&lt;/pre&gt;
&lt;p&gt;Additionally, histograms can be generated and co-occurrence data between
patterns can be shown, which is computed using normalised pointwise mutual information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="relations"&gt;
&lt;h3&gt;Relations&lt;/h3&gt;
&lt;p&gt;Various relations can be extracted from patterns in an indexed pattern model:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Subsumption relations&lt;/strong&gt; - Patterns that are subsumed by larger patterns are called subsumption children, the larger patterns are called subsumption parents. These are the two subsumption relations that can be extracted from an indexed pattern model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Successor relations&lt;/strong&gt; - Patterns that follow eachother are in a left-of/right-of relation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instantiation relations&lt;/strong&gt; - There is a relation between skipgrams and patterns that instantiate them: &lt;tt class="docutils literal"&gt;to be {*1*} not {*1*} be&lt;/tt&gt; is instantiated by &lt;tt class="docutils literal"&gt;to {*1*} or&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A web-based tool is available that allows these relations to be visualised and navigated interactively.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="manipulating-models"&gt;
&lt;h3&gt;Manipulating models&lt;/h3&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 143)&lt;/p&gt;
&lt;p&gt;Title underline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Manipulating models
-----------
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Models can be actively manipulated by filtering and intersecting them with
other models. They can moreover be manipulated at a more finegrained level from
Python scripts using the available Python binding.&lt;/p&gt;
&lt;p&gt;A very common application of intersecting models is when a first pattern model is
generated on a training corpus, and subsequently a second model is generated on
test data, but constrained using the training model. This results in a model
that contains only all patterns occurring both in training and test data, the
coverage metric of such a model thus provides a rough measure of overlap between corpora.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="implementation"&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 155)&lt;/p&gt;
&lt;p&gt;Title overline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
=====
Implementation
=====
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Colibri Core is entirely implemented in C++, to ensure efficiency with regard
to speed and memory. A full Python binding, directly interfacing with the native code,
is available, allowing you to load, query and manipulate Pattern Models from
Python scripts (version 3 is supported).&lt;/p&gt;
&lt;p&gt;The software offers a powerful command-line tool, designed for Unix-based
systems such as Linux.&lt;/p&gt;
&lt;div class="section" id="under-the-hood"&gt;
&lt;h3&gt;Under the hood&lt;/h3&gt;
&lt;div class="system-message"&gt;
&lt;p class="system-message-title"&gt;System Message: WARNING/2 (&lt;tt class="docutils"&gt;/home/proycon/work/contemplativecoding/content/2014-03-31-colibri-core.rst&lt;/tt&gt;, line 168)&lt;/p&gt;
&lt;p&gt;Title underline too short.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Under the hood
-----
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The API of Colibri Core offers an ideal foundation for building NLP software
that works on large numbers of patterns, and which seeks to constrain memory
usage and attaining high-performance.&lt;/p&gt;
&lt;p&gt;In addition to offering a base class that holds a pattern, Colibri Core offers
a variety of containers (for C++ programmers; these behave like C++11 STL containers)
for storing patterns. The current storage backends are based on hashmaps or
ordered maps, but additional storage backends are envisioned for the future, such as
prefix tries/suffix tries or suffix arrays.&lt;/p&gt;
&lt;p&gt;A schematic overview of the Colibri Core architecture is available below, consult the
documentation at &lt;a class="reference external" href="http://proycon.github.io/colibri-core/doc/"&gt;http://proycon.github.io/colibri-core/doc/&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;image::http://proycon.github.io/colibri-core/doc/images/arch.png&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</summary><category term="ngrams"></category><category term="skipgrams"></category><category term="python"></category></entry><entry><title>And so it begins...</title><link href="http://proycon.github.io/blog/2013/11/17/new-blog/" rel="alternate"></link><updated>2013-11-12T14:01:01+01:00</updated><author><name>Maarten van Gompel (proycon)</name></author><id>tag:proycon.github.io,2013-11-17:blog/2013/11/17/new-blog/</id><summary type="html">&lt;p&gt;Welcome to my new blog! Here I will talk about software development and show
some of the things I am working on. At times it is good to take some distance
from coding, and contemplate what you have written. In doing so, I can
hopefully offer interesting open source tools and ideas to the community.&lt;/p&gt;
&lt;p&gt;A small introduction may be in order for those who don't know me yet. My name
is &lt;em&gt;Maarten van Gompel&lt;/em&gt; (aka proycon). I am a PhD researcher and scientific
programmer at the Centre for Language Studies, Radboud University Nijmegen. I
love both languages as well as technology and I am happily working in the field
where these two independent interests meet: &lt;em&gt;Natural Language Processing&lt;/em&gt; or &lt;em&gt;Computational Linguistics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;My PhD project &lt;em&gt;Colibri&lt;/em&gt; focusses on the role of constructions in Machine Translation, as well
as related techniques such as Cross-Lingual Word Sense Disambiguation.&lt;/p&gt;
&lt;p&gt;I'm an avid open source advocate and linux fan. My tools of the trade are
mostly Python and C++, sometimes along with web technologies such as Django,
CSS, HTML, Javascript/Jquery.&lt;/p&gt;
&lt;p&gt;Starting a blog always comes with the obvious but daunting obligation of
maintaining it. Expect to see an occasional, like bi-monthly, posts. So keep
posted!&lt;/p&gt;
</summary><category term="intro"></category></entry></feed>